# REPLACE THE weights_directory WITH THE DIRECTORY WHERE YOU STORE MODEL WEIGHTS, RELATIVE TO generate_acts.py
[Llama3]
weights_directory = meta-llama/Meta-Llama-3-8B-Instruct 
8B_base_subdir = llama3_8b_hf
8B_chat_subdir = llama3_8b_chat_hf
70B_base_subdir = llama3_70b_hf
70B_chat_subdir = llama3_70b_chat_hf

[Llama3.1] 
weights_directory = meta-llama/Llama-3.1-8B-Instruct
8B_base_subdir = llama3.1_8b_hf 
8B_chat_subdir = llama3.1_8b_chat_hf 
70B_base_subdir = llama3.1_70b_hf 
70B_chat_subdir = llama3.1_70b_chat_hf

[Llama2]
weights_directory = ../../../../data/lbuerger/llama_hf 
7B_base_subdir = llama2_7b_hf
7B_chat_subdir = llama2_7b_chat_hf
13B_base_subdir = llama2_13b_hf
13B_chat_subdir = llama2_13b_chat_hf
70B_base_subdir = llama2_70b_hf
70B_chat_subdir = llama2_70b_chat_hf

[Gemma]
weights_directory = ../../../../data/lbuerger/gemma
7B_base_subdir = gemma_7b_hf
7B_chat_subdir = gemma_7b_chat_hf

[Gemma2]
weights_directory = google/gemma-2-2b-it
27B_chat_subdir = gemma2_27b_it_hf
9B_base_subdir = gemma2_9b_hf

[Mistral]
weights_directory = mistralai/Mistral-7B-Instruct-v0.3
7B_chat_subdir = mistral_7b_it_hf_v3

[Qwen2.5]
weights_directory = Qwen/Qwen2.5-7B-Instruct
7B_chat_subdir = qwen2.5_7b_it

[DeepSeek]
weights_directory = deepseek-ai/DeepSeek-R1-Distill-Llama-8B